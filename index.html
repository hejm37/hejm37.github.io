<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jiamin He</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">

  <meta name="author" content="Jiamin He">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
  <link rel="stylesheet" type="text/css" href="css/style.css">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

  <!-- Begin ignoring several indetation. -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:2.5%;width:70%;vertical-align:middle">
          <p style="text-align:center">
            <name>Jiamin He</name>
          </p>

          <p>
            I am a Ph.D. student in Computing Science at the <a href="https://www.ualberta.ca/">University of Alberta</a>, supervised by <a href="https://marthawhite.ca/">Martha White</a>. Currently, I'm interning at <a href="https://deepmind.google/">Google DeepMind</a> in London, working with <a href="https://scholar.google.com/citations?user=LK_CV24AAAAJ&hl">Diana Borsa</a> and <a href="https://hadovanhasselt.com/">Hado van Hasselt</a> on foundational reinforcement learning algorithms and their applications in science.
          </p>

          <p>
            Previously, I received my M.Sc. (Thesis) degree in Computing Science at the University of Alberta under the supervision of <a href="https://armahmood.github.io/">Rupam Mahmood</a>. Before that, I also worked with <a href="https://mig-ai.github.io/person-zhangchongjie.html">Chongjie Zhang</a> as a research intern.
          </p>

          <p>
            My research interests lie in reinforcement learning, with a focus on policy optimization, off-policy learning, and representation learning.
          </p>

          <p style="text-align:center">
            <a href="mailto:hejm37@gmail.com">Email</a> &nbsp/&nbsp
            <a href="https://scholar.google.ca/citations?user=hZxi1YcAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
            <a href="https://github.com/hejm37">GitHub</a> &nbsp/&nbsp
            <a href="http://agtsmith.com/">Blog</a>
          </p>
        </td>

        <td style="padding:2.5%;width:300%;max-width:300%">
          <a href="images/photo_jiamin_yeg.png"><img style="width:110%;max-width:110%;border-radius:15%"
              alt="profile photo" src="images/photo_jiamin_yeg.png" class="hoverZoomLink"></a>
        </td>
      </tr>
    </tbody>
  </table>


  <hr>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Publications</heading>
        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>Distributions as Actions: A Unified Framework for Diverse Action Spaces.</papertitle>
            <br>
            <b>Jiamin He</b>, A. Rupam Mahmood, Martha White.
            <br>
            <em><a href="https://openreview.net/forum?id=KWZZtAOE1I" style="color: grey;">Preliminary version at the Finding the Frame Workshop at RLC</em>, 2025.</a>
            <br>
            <em>ICLR</em>, 2026.
            <br>
            Paper and code coming soon.
            <!-- <a href="https://openreview.net/forum?id=KWZZtAOE1I">workshop version</a> -->
          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>Investigating the Utility of Mirror Descent in Oﬀ-policy Actor-Critic.</papertitle>
            <br>
            Samuel Neumann, <b>Jiamin He</b>, Adam White, Martha White.
            <br>
            <em>RLC</em>, 2025.
            <br>
            <a href="https://openreview.net/forum?id=A3RiCiOE8f">paper</a> | <a href="https://github.com/samuelfneumann/MirrorDescentRL">code</a>
          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>Deep Policy Gradient Methods Without Batch Updates, Target Networks, or Replay Buffers.</papertitle>
            <br>
            Gautham Vasan, Mohamed Elsayed, Alireza Azimi*, <b>Jiamin He*</b>, Fahim Shariar, Colin Bellinger, Martha White, A. Rupam Mahmood.
            <br>
            <em>NeurIPS</em>, 2024.
            <br>
            <a href="https://openreview.net/forum?id=DX5GUwMFFb">paper</a> | <a href="https://github.com/gauthamvasan/avg">code</a>
          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>Loosely Consistent Emphatic Temporal-Difference Learning.</papertitle>
            <br>
            <b>Jiamin He</b>, Fengdi Che, Wan Yi, A. Rupam Mahmood.
            <br>
            <em><a href="https://openreview.net/forum?id=V0vLQvqCkpY" style="color: grey;">Preliminary version in the average-reward setting at the Deep RL Workshop at NeurIPS</em>, 2022.</a>
            <br>
            <em>UAI</em>, 2023.
            <br>
            <a href="https://proceedings.mlr.press/v216/he23a.html">paper</a> | <a href="https://github.com/hejm37/LC-ETD">code</a>
          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>Episodic Multi-agent Reinforcement Learning with Curiosity-Driven Exploration.</papertitle>
            <br>
            Lulu Zheng*, Jiarui Chen*, Jianhao Wang, <b>Jiamin He</b>, Yujing Hu, Yingfeng Chen, Changjie Fan, Yang Gao,
            Chongjie Zhang.
            <br>
            <em>NeurIPS</em>, 2021.
            <br>
            <a href="https://proceedings.neurips.cc/paper/2021/hash/1e8ca836c962598551882e689265c1c5-Abstract.html">paper</a> | <a href="https://github.com/kikojay/EMC">code</a>
            <!-- <br>
            <font color="black">
              <p>
                We introduce a novel Episodic Multi-agent reinforcement learning with
                Curiosity-driven exploration, called EMC. We leverage an insight of popular factorized MARL algorithms
                that the "induced" individual Q-values, i.e., the individual utility functions used for local execution,
                are the embeddings of local action-observation histories, and can capture the interaction between agents
                due to reward backpropagation during centralized training. Therefore, we use prediction errors of
                individual Q-values as intrinsic rewards for coordinated exploration and utilize episodic memory to
                exploit explored informative experience to boost policy training.
              </p>
            </font>
            <br> -->
          </p>
        </td>
      </tr>

    </tbody>
  </table>


  <hr>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Other Workshop Papers</heading>
        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>Revisiting Mixture Policies in Entropy-Regularized Actor-Critic.</papertitle>
            <br>
            <b>Jiamin He</b>, Samuel Neumann, Jincheng Mei, Adam White, Martha White.
            <br>
            <em>Aligning Reinforcement Learning Experimentalists and Theorists Workshop at NeurIPS</em>, 2025.
            <br>
            <em>Extended version under review</em>, 2026.
            <br>
            <a href="https://openreview.net/forum?id=TImqVqOQuC">paper</a>
          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>Improving Reward-Based Hindsight Credit Assignment.</papertitle>
            <br>
            Aditya A. Ramesh, <b>Jiamin He</b>, Jürgen Schmidhuber, Martha White.
            <br>
            <em>European Workshop on Reinforcement Learning</em>, 2025.
            <br>
            <a href="https://openreview.net/forum?id=2tKj9WC9YQ">paper</a>
          </p>
        </td>
      </tr>

    </tbody>
  </table>


  <hr>

<!-- 
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Preprints</heading>
        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>Hindsight Multi-agent Credit Assignment.</papertitle>
            <br>
            <b>Jiamin He</b>, Weijun Dong, Yiming Lu, Hao Hu, Chongjie Zhang.
            <br>
            <em>Preprint</em>, 2021.
            <br>
            <a href="https://hejm37.github.io/files/hmaca_2021.pdf">paper</a>
          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>Genre-based Reinforcement Learning Recommender System.</papertitle>
            <br>
            <b>Jiamin He</b>.
            <br>
            <em>Preprint</em>, 2019.
            <br>
            <a href="https://hejm37.github.io/files/grlrs_2019.pdf">paper</a>
          </p>
        </td>
      </tr>

    </tbody>
  </table>


  <hr> -->


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Theses</heading>
        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>Consistent Emphatic Temporal-Difference Learning.</papertitle>
            <br>
            <b>Jiamin He</b>.
            <br>
            <em>M.Sc. Thesis, University of Alberta</em>, 2023.
            <br>
            <a href="https://era.library.ualberta.ca/items/f911bb7e-317e-424d-8b68-f8b2eae42fc2">details</a>
          </p>
        </td>
      </tr>

    </tbody>
  </table>

  <hr>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px">
          <br>
          <p style="text-align:right;">Stolen from <a href="https://jonbarron.info/">Jon Barron</a></p>
        </td>
      </tr>
    </tbody>
  </table>

  <!-- End ignoring several indetation. -->
        </td>
      </tr>
    </tbody>
  </table>

</body>


</html>