<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jiamin He</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">

  <meta name="author" content="Jiamin He">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="css/stylesheet.css">
  <link rel="stylesheet" type="text/css" href="css/style.css">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

  <!-- Begin ignoring several indetation. -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:2.5%;width:70%;vertical-align:middle">
          <p style="text-align:center">
            <name>Jiamin He</name>
          </p>

          <p>
            I'm a Computer Science M.Sc. (Thesis) student at the University of Alberta. I am supervised by <a href="https://armahmood.github.io/">Rupam Mahmood</a>. My current research interests lie in artificial intelligence, especially reinforcement learning.
          </p>

          <p>
            I got my Bachelor's degree in Information and Computing Science at Sun Yat-sen University under the
            supervision of <a href="https://www.xplan-lab.org/hankz/">Hankz Hankui Zhuo</a>.
            I was previously a research intern at Machine Intelligence Group at Tsinghua University led by
            <a href="http://people.iiis.tsinghua.edu.cn/~zhang/">Chongjie Zhang</a>.
          </p>

          <p style="text-align:center">
            <a href="mailto:hejm37@gmail.com">email</a> &nbsp/&nbsp
            <a href="https://github.com/hejm37">GitHub</a> &nbsp/&nbsp
            <a href="http://agtsmith.com/">blog</a>
          </p>
        </td>

        <td style="padding:2.5%;width:300%;max-width:300%">
          <a href="images/photo_jiamin_ski.png"><img style="width:110%;max-width:110%;border-radius:15%"
              alt="profile photo" src="images/photo_jiamin_ski.png" class="hoverZoomLink"></a>
        </td>
      </tr>
    </tbody>
  </table>


  <hr>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Publications</heading>
        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>The Emphatic Approach to Average-Reward Policy Evaluation.</papertitle>
            <br>
            <b>Jiamin He</b>, Wan Yi, A. Rupam Mahmood.
            <br>
            <em>Deep Reinforcement Learning Workshop at NeurIPS</em>, 2022.
            <br>
            <a href="https://openreview.net/forum?id=V0vLQvqCkpY">paper</a>
          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>Hindsight Multi-agent Credit Assignment.</papertitle>
            <br>
            <b>Jiamin He</b>, Weijun Dong, Yiming Lu, Hao Hu, Chongjie Zhang.
            <br>
            <em>In Revision</em>, 2021.
          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <papertitle>Episodic Multi-agent Reinforcement Learning with Curiosity-Driven Exploration.</papertitle>
            <br>
            Lulu Zheng*, Jiarui Chen*, Jianhao Wang, <b>Jiamin He</b>, Yujing Hu, Yingfeng Chen, Changjie Fan, Yang Gao,
            Chongjie Zhang.
            <br>
            <em>Conference on Neural Information Processing Systems (NeurIPS)</em>, 2021.
            <br>
            <a href="https://arxiv.org/abs/2111.11032">paper</a> | <a href="https://github.com/kikojay/EMC">code</a>
            <!-- <br>
            <font color="black">
              <p>
                We introduce a novel Episodic Multi-agent reinforcement learning with
                Curiosity-driven exploration, called EMC. We leverage an insight of popular factorized MARL algorithms
                that the "induced" individual Q-values, i.e., the individual utility functions used for local execution,
                are the embeddings of local action-observation histories, and can capture the interaction between agents
                due to reward backpropagation during centralized training. Therefore, we use prediction errors of
                individual Q-values as intrinsic rewards for coordinated exploration and utilize episodic memory to
                exploit explored informative experience to boost policy training.
              </p>
            </font>
            <br> -->
          </p>
        </td>
      </tr>

    </tbody>
  </table>


  <hr>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Projects</heading>
        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>
            <b>
              <papertitle>Hindsight Multi-agent Credit Assignment</papertitle>
            </b>
            <br>
            <font color="black">
              Vanilla policy gradient methods for MARL suffer from both variance and lack of counterfactuals, making the learning process data inefficient and unstable. We proposed a unified credit assignment framework to simultaneously assign credit over time and agents to address these issues. We proposed the Hindsight Multi-Agent Credit Assignment (HMACA), which uses both hindsight likelihood and counterfactual reasoning to assign credit over timesteps and agents.
            </font>
          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:30%;vertical-align:middle">
          <p>
            <b>
              <papertitle>Emergent Tool Use in Multi-agent Reinforcement Learning</papertitle>
            </b>
            <br>
            <font color="black">
              We used PPO and adversarial training to train two teams of agents in a mixed cooperative-competitive asymmetric game environment we developed. During training, the agents learned to use ladders to cross the walls or block the adversaries, and surprisingly, they also learned to use ladders to speed up even if there were no walls or adversaries nearby. To make learning more efficient, we also adopted Hindsight Credit Assignment to address the temporal credit assignment.
            </font>
            <br>
            <a href="https://youtu.be/6MjTkEFH6nA">demo</a>
          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:30%;vertical-align:middle">
          <p>
            <b>
              <papertitle>Sparkle Planning Challenge 2019 Entry: SYSU-Planner</papertitle>
            </b>
            <br>
            <font color="black">
              We combined two planners into a two-phased planner that performs a fast best-first width search followed by a refinement hill-climbing search and obtained better overall performance. We achieved state-of-the-art of classical planning in 1/3 of the tested domains and outperformed all previous methods in a few domains. Our planner is in third place on the leaderboard and fourth place in the final result.
            </font>
            <br>
            <a
              href="https://ada.liacs.nl/events/sparkle-planning-19/documents/solver_description/SYSU-planner-description.pdf">description</a>
            |
            <a href="https://ada.liacs.nl/events/sparkle-planning-19/leaderboard.html">leaderboard</a>
          </p>
        </td>
      </tr>

    </tbody>
  </table>


  <hr>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Experience</heading>
        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <ul>
          <p>
            <li> <b>2021.01 ~ 2021.08: Multi-agent Reinforcement Learning Research (Tsinghua University)</b>
              <br>
              I focused on multi-agent reinforcement learning during my stay at Tsinghua University. More specifically, I worked on multi-agent exploration and multi-agent credit assignment (See my publication <em>Episodic Multi-agent Reinforcement Learning with Curiosity-Driven Exploration</em> and project <em>Hindsight Multi-agent Credit Assignment</em> above).
              </br>
            </li>
          </p>

          <p>
            <li> <b>2020.09 ~ 2020.12: Multi-agent Reinforcement Learning Research (Parametrix.ai)</b>
              <br>
              I worked on game AI for a mixed cooperative-competitive asymmetric game environment, training agents to cooperate and defeat the other team. Our agents emerge with interesting tool-use behaviors during training. I also tried to use Hindsight Credit Assignment to address the temporal credit assignment, and later I developed the idea into <em>Hindsight Multi-agent Credit Assignment</em> (see Projects).
              </b>
            </li>
          </p>

          <p>
            <li> <b>2019.10 ~ 2020.07: Image Matting and Video Matting (SenseTime)</b>
              <br>
              During my time at SenseTime, I was supervised by <a href="https://chenkai.site/">Kai Chen</a>, working on image matting and video matting. I developed a pipeline (with segmentation, morphological transformation, and matting) for industrial green screen video matting. The technique was integrated into <a href="https://www.sensetime.com/en/news-detail/54623?categoryId=1072">SenseNeo</a>, the AI-generated content advertising platform of SenseTime. I also developed MMEditing with <a href="https://xinntao.github.io/">Xintao Wang</a> and <a href="https://nbei.github.io/">Rui Xu</a>. MMEditing is an open source image and video editing toolbox. It now has over 3.8k stars on GitHub. <a href="https://github.com/open-mmlab/mmediting">Check it out!</a>

              <!-- I give a talk about <a href="files/matting.pdf">matting</a> in SenseTime. -->
              </br>
            </li>
          </p>

        </ul>
      </tr>
    </tbody>
  </table>


  <hr>

<!-- 
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading>Teaching and Service</heading>
        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>

          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>

          </p>
        </td>
      </tr>

      <tr bgcolor="#ffffff">
        <td style="padding:20px;width:100%;vertical-align:middle">
          <p>

          </p>
        </td>
      </tr>

    </tbody>
  </table>


  <hr>
 -->

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px">
          <br>
          <p style="text-align:right;">Stolen from <a href="https://jonbarron.info/">Jon Barron</a></p>
        </td>
      </tr>
    </tbody>
  </table>

  <!-- End ignoring several indetation. -->
        </td>
      </tr>
    </tbody>
  </table>

</body>


</html>